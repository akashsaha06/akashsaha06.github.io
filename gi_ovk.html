<!DOCTYPE html>
<!--
    Plain-Academic by Vasilios Mavroudis
    Released under the  Simplified BSD License/FreeBSD (2-clause) License.
    https://github.com/mavroudisv/plain-academic
-->

<html lang="en">
<head>
    <title>Akash Saha</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Oswald:700' rel='stylesheet' type='text/css'>
    <link rel="icon" type="image/x-icon" href="icons/iitb_logo.png">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>


<body>

    <!-- Navigation -->
    <nav class="navbar navbar-inverse">
        <div class="container">
            <ul class="nav navbar-nav">
                <li><a href="index.html">Home</a></li>
                <li><a style="color:#e01709" href="experience.html">Experience</a></li>
                <li><a href="publications.html">Publications</a></li> 
                <li><a href="docs/Akash_Saha_Resume.pdf" target="_blank">Resume</a></li> 
            </ul>
        </div>
    </nav>
    

    <!-- Page Content -->
    <div class="container">

        <div class="row">

            <!-- Experience -->
            <div class="col-md-8" style="min-height: 100vh; height: auto;"> <!--100hv-->
                <h2 id="exp">Learning Sparse Graphs for Functional Regression using Graph-induced Operator-valued Kernels</h2>
                <p align="justify">
                    Learning to predict functional output from a suitable input is characterized as a functional regression problem, which aims at learning a function-valued function \(F:\mathcal{Z}\rightarrow\mathcal{Y}\), where \(\mathcal{Z}\) is an appropriate input space and \(\mathcal{Y}\) is an output space of functions. 
                    In many scenarios, multiple inputs decide the value of an output, which gives rise to functional regression problems of the form \(F:\mathcal{Z}^p\to\mathcal{Y}\), where \(p\) is the number of inputs considered. 
                    Even more interesting is the case where interactions among the \(p\) inputs can be used in a precise manner to predict \(y\in\mathcal{Y}\). 
                    In particular, we consider \(\mathcal{Z}\) to be a space of functions, hence learning a map \(F:\mathcal{Z}^p\to\mathcal{Y}\) is called a many-to-one function-to-function regression problem. 
                    Without loss of generality, we refer to this many-to-one function-to-function regression problem as the functional regression problem considered throughout this paper. 
                    Applications of this type of problems can be found in weather forecasting where different weather parameters in stations measured at multiple timepoints across a month can be characterized as functional inputs used to determine the average rainfall as a time-varying function in that month. 
                    Similarly, emissions from a factory in a day can be predicted as a function of time, based on the functional data obtained from readings of different components involved in the manufacturing process at different timepoints in that day. 
                    In sports analytics, the movement data of different players throughout the game can let us know the influence of a particular strategy in ball possession/movement as a functional output over the duration of the game. 
                    Thus in all these applications, we notice situations where a set of input functions interact to produce an output function. 
                    Even though digital data is discrete, systems where the inherent data produced is smooth and continuous by nature, can be modeled as functions over a suitable domain to leverage the variations based on that domain.<br>
                </p>
                    <center>
                        <figure>
                            <img src="motivation_diag.png"
                                 width="800" 
                                 height="600" />
                            <figcaption id="motivation_diag_fig"> Illustrative example of a functional regression problem, where \(z_1,z_2,z_3\) represent atmospheric pressure at 3 stations in a region, graph \(G\) depicts the inter-relationships among \(z_1,z_2,z_3\) and \(y\) represents the average temperature of the region. \(F\) maps \(z_1,z_2,z_3\) to \(y\) incorporating \(G\).</figcaption>
                          </figure><br>
                    </center>
                <p align="justify">
                    Consider a simple functional regression problem illustrated in <a href="#motivation_diag_fig"></a>, where input functions \(z_1,z_2,z_3\in\mathcal{Z}\) denote the atmospheric pressure measurements of 3 nearby weather stations and the output function \(y\in\mathcal{Y}\) denotes the average temperature of the region, throughout a particular day. 
                    For predicting \(y\), considering the input functions \(z_1,z_2\) and \(z_3\) without any relation among them may be restrictive as inherent relations between the input functions may dictate the generation of \(y\). 
                    In order to capture interactions among \(z_1,z_2,z_3\), we introduce a graph structure \(G\) between \(z_1,z_2\) and \(z_3\) in <a href="#motivation_diag_fig"></a>, where the nodes of \(G\) represent \(z_i\)'s and the edges depict potential relations among them. 
                    The graph structure \(G\) will be useful in representing the influences and inter-relations among \(z_i\)'s, which can be useful in the prediction of \(y\in\mathcal{Y}\) using \(F\). 
                    We propose a framework for combining the impact of \(z_1,z_2,\dots,z_p\) with the additional information of \(G\) to predict \(y\). 
                    In determining the output function \(y\), the graphical structure \(G\) on the input functions may be known from domain knowledge and can possibly be directly incorporated to learn \(F\). 
                    A more interesting case is when \(G\) is unknown and needs to be learned along with \(F\). 
                    Learning the graph structure \(G\) would help to discover interactions among \(z_i\)'s which facilitate predicting \(y\). 
                    When the number of input functions \(z_1,z_2,\dots,z_p\) grow larger, the associated graph \(G\) might also become dense with many edges and incorporating such dense \(G\) might lead to computational difficulties and would also lead to spurious connections/edges which lack interpretability. 
                    Thus, learning a sparse graphical structure \(G\) on input functions becomes instrumental in understanding the significant relationships that drive the functional regression problem to predict the output function.<br>
                </p>
                <p align="justify">
                    A functional regression problem is based on learning a function \(\mathcal{F}\), where \(\mathcal{F}:\mathcal{X}\rightarrow\mathcal{Y}\), \(\mathcal{X}\) is an appropriate input space and \(\mathcal{Y}\) is an output space of functions. 
                    Functional regression problems (<a href="#func_reg_fig"></a>) find applications in audio-visual tasks and weather forecasting, etc. 
                    Scalar-valued kernels \(k\), such that \(k(.,.):X\times X\rightarrow\mathbb{R}\) have been a popular tool for machine learning tasks where elements (e.g. vectors) from the input space \(X\) are mapped to a reproducing kernel Hilbert spaces (RKHS) which simplifies the task of obtaining a relationship between them. 
                    Operator-valued kernels present an extension to scalar-valued kernels such that they map two elements from the input space to a bounded linear operator on the output space. 
                    An operator-valued kernel can be mathematically defined as \(K(.,.):\mathcal{X}\times\mathcal{X}\rightarrow\mathcal{L(Y)}\).<br>
                </p>
                    <center>
                        <figure>
                            <img src="func_reg.PNG" alt="func_regression">
                            <figcaption id="func_reg_fig"> Illustration of a functional regression problem.</figcaption>
                          </figure><br>
                    </center>
                <p align="justify">
                    Operator-valued kernels have shown promise in the field of functional regression problems. 
                    Similar to scalar-valued kernels, one of the fundamental properties of operator-valued kernels is positive semi-definiteness which ensures a bijection between the positive semi-definite kernels and the associated RKHS [<a href="https://www.jmlr.org/papers/volume17/11-315/11-315.pdf">Kadri et al., 2016</a>]. 
                    A relaxation of the positive semi-definiteness property of scalar-valued kernels has been explored [<a href="https://dl.acm.org/doi/pdf/10.1145/1015330.1015443">Ong et al., 2004</a>], where non-positive kernels have been used to map a pair of elements in the input space to an associated reproducing kernel Krein space (RKKS). 
                    A Krein space is a direct sum of two orthogonal Hilbertian subspaces with respect to a bilinear form instead of an inner product. 
                    In this work, we consider a special category of operator-valued kernels called generalized operator-valued kernels which might not be necessarily positive semi-definite. 
                    <a href="#rkhs_fig"></a> illustrates the elements of the input space being mapped into the associated RKKS corresponding to a generalized operator-valued kernel and to an associated RKHS corresponding to an operator-valued kernel.<br>
                    We derive a result (Theorem 2.4 [<a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/9f319422ca17b1082ea49820353f14ab-Paper.pdf">Saha & Palaniappan, 2020</a>]) which ensures that given a generalized operator-valued kernel there exists an associated RKKS where the learning problem can be formulated. 
                    Based on the result above, we can establish that for a generalized operator-valued kernel \(\breve{K}\), we obtain an associated RKKS where the learning problem can be formulated.
                </p>
                    <center>
                        <figure>
                            <img src="rkhs.PNG" alt="rkhs">
                            <figcaption id="rkhs_fig"> Use of operator-valued kernel with its associated RKHS and generalized operator-valued kernel with its associated RKKS.</figcaption>
                          </figure><br>
                    </center>
                <p align="justify">
                    For the learning problem formulation, we consider a regularized loss stabilization problem consisting of a loss term and a bilinear form term.
                    A minimization problem is avoided owing to the possible negativity of the bilinear form in the associated RKKS \(\mathcal{K}\).
                    The loss stabilization problem essentially finds a stationary point which may not be a minimization point.<br>
                </p>
                    <center>
                        <figure>
                            <img src="motiv.PNG" alt="motiv">
                            <figcaption id="motiv_fig"> Motivation for function-valued RKKS.</figcaption>
                          </figure><br>
                    </center>
                <p align="justify">
                    A representer theorem (Theorem 3.1 [<a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/9f319422ca17b1082ea49820353f14ab-Paper.pdf">Saha & Palaniappan, 2020</a>]) for the loss stabilization problem provides a representation of the required stabilizer which can be obtained by solving a linear operator system.
                    The linear operator system involves block operator kernel matrix which encodes the relationship between input and output functions in training data. 
                    A proposed iterative operator-based minimum residual method called OpMINRES solves the linear operator system providing basis functions which can be used with the representer theorem to obtain the learned functional. 
                    In OpMINRES algorithm, we incrementally build subspaces where residual norm minimization is performed in each iteration.<br><br>

                    In the experiments, lip aperture functions were used for speech inversion where an audio clip is used to predict the corresponding lip aperture function. 
                    We use residual sum of squares error which is suited for functional regression problems in our experiments. 
                    Various combinations of input and output kernels were considered, using difference of kernels in one of them. 
                    We used OpMINRES with various combinations of input and output kernels to create generalized operator-valued kernels. 
                    An experiment was considered using Diffusion Tensor Imaging (DTI) dataset, where we intend to figure out the relation between two FA (fractional anisotropy) tract profiles for both healthy and unhealthy individuals. 
                    The results obtained in each of the experiments were competitive with the benchmark methods. 
                    Other ways to create generalized operator-valued kernel and using them for specific functional regression problems is a possible future research area.
                </p>
                <script>
                    $(document).ready(function(){
                        $("figcaption").each(function(index,value){
                            $(this).prepend("Figure "+(++index)+". ");
                            $('[href="#'+$(this).attr("id")+'"]').text("Figure "+index)
                        });
                    });
                </script>
                
            </div>

            <!-- Contact Info on the Sidebar -->
            <div class="col-md-4">
                <div style="font-family:'Oswald',sans-serif; font-size: 32px;">
                    <a style="color:#595d61;" href="index.html"><b>Akash Saha</b></a>
                </div>
                <br>
                <p><b>akashsaha@iitb.ac.in</b><br>
                    <b>akashsaha06@gmail.com</b>
                <p>
                    PhD Student<br>
                    IEOR<br>
                    IIT Bombay, Mumbai<br>
                </p>
                <a style="color:#333;" href="https://www.linkedin.com/in/akash-saha-b820b149/" target="_blank"><img src="icons/LinkedIn.png" width="30" height="30"></a>
                &nbsp;
                <a style="color:#333;" href="https://scholar.google.co.in/citations?user=lB5LhOQAAAAJ&hl=en" target="_blank"><img src="icons/Gscholar.png" width="30" height="30"></a>
                &nbsp;
                <a style="color:#333;" href="https://github.com/akashsaha06" target="_blank"><img src="icons/GitHub.png" width="30" height="30"></a>
            </div>
        </div>

    </div>
    <!-- /.container -->

    
    <!-- Other people may like it too! -->
    <a style="color:#868d96;font-size:0.9em; float:right;" href="https://github.com/mavroudisv/plain-academic" target="_blank">Design courtesy of Vasilios Mavroudis: Plain Academic &emsp;</a><br>
    
</body>


</html>
